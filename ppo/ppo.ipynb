{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ppo.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Proximal Policy Optimization (PPO) playground\n",
    "\n",
    "Notebook for running PPO on simple environments from OpenAI Gym"
   ],
   "metadata": {
    "id": "LmQoT8K6nJX_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install dependencies (only on Google Colab)"
   ],
   "metadata": {
    "id": "bAZM8OR8nm2-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Installing our own implementation\n",
    "! git clone https://github.com/emasquil/ppo.git -b main-notebook\n",
    "! pip install -e /content/ppo\n",
    "\n",
    "# Visualization stuff\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install pyvirtualdisplay"
   ],
   "metadata": {
    "id": "9avxJbcjt8Oi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from acme import specs\n",
    "from acme.utils import loggers\n",
    "import pyvirtualdisplay\n",
    "\n",
    "# Set up a virtual display for rendering.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "\n",
    "import ppo.dm_helper as helpers\n",
    "from ppo.agents import RandomAgent\n",
    "from ppo.env_wrapper import PendulumEnv"
   ],
   "metadata": {
    "id": "FHET6yCkw4ds"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization functions"
   ],
   "metadata": {
    "id": "E7_OrJQ0qls7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def display_video(frames, filename=\"temp.mp4\", frame_repeat=1):\n",
    "    \"\"\"Save and display video.\"\"\"\n",
    "    # Write video\n",
    "    with imageio.get_writer(filename, fps=60) as video:\n",
    "        for frame in frames:\n",
    "            for _ in range(frame_repeat):\n",
    "                video.append_data(frame)\n",
    "    # Read video and display the video\n",
    "    video = open(filename, \"rb\").read()\n",
    "    b64_video = base64.b64encode(video)\n",
    "    video_tag = (\n",
    "        '<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
    "        'src=\"data:video/mp4;base64,{0}\">'\n",
    "    ).format(b64_video.decode())\n",
    "    return IPython.display.HTML(video_tag)"
   ],
   "metadata": {
    "id": "SOhF8c-AqlTa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definitions\n",
    "\n",
    "Definition of all the parts used in the learning loop: environment, agent, etc."
   ],
   "metadata": {
    "id": "Q-40V1lKDlip"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "ENV_NAME = \"Pendulum-v1\""
   ],
   "metadata": {
    "id": "y7uMVaYaFcA0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create an environment, grab the spec\n",
    "# We should use our own environment here\n",
    "environment = helpers.make_environment(ENV_NAME)\n",
    "environment_spec = specs.make_environment_spec(environment)\n",
    "\n",
    "# environment = PendulumEnv()\n",
    "# environment_spec = specs.make_environment_spec(environment)"
   ],
   "metadata": {
    "id": "rltCiwH2FYWA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the networks\n",
    "# We should use our own netowkrs here\n",
    "# agent_networks = dm_ppo.make_gym_networks(environment_spec)"
   ],
   "metadata": {
    "id": "kfJrCivxHmqH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the agent\n",
    "# We should use PPO agent\n",
    "agent = RandomAgent(environment_spec)"
   ],
   "metadata": {
    "id": "xpJvcAOtF2t3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interaction loop"
   ],
   "metadata": {
    "id": "kluuB3xPJd_u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def training_loop(\n",
    "    environment,\n",
    "    agent,\n",
    "    num_episodes=None,\n",
    "    num_steps=None,\n",
    "    logger_time_delta=1.0,\n",
    "    label=\"training_loop\",\n",
    "):\n",
    "    \"\"\"Perform the run loop.\n",
    "\n",
    "    We are following the Acme run loop.\n",
    "\n",
    "    Run the environment loop for `num_episodes` episodes. Each episode is itself\n",
    "    a loop which interacts first with the environment to get an observation and\n",
    "    then give that observation to the agent in order to retrieve an action. Upon\n",
    "    termination of an episode a new episode will be started. If the number of\n",
    "    episodes is not given then this will interact with the environment\n",
    "    infinitely.\n",
    "\n",
    "    Args:\n",
    "      environment: dm_env used to generate trajectories.\n",
    "      agent: acme.Actor for selecting actions in the run loop.\n",
    "      num_steps: number of episodes to run the loop for. If `None` (default), runs\n",
    "        without limit.\n",
    "      num_episodes: number of episodes to run the loop for. If `None` (default),\n",
    "        runs without limit.\n",
    "      logger_time_delta: time interval (in seconds) between consecutive logging\n",
    "        steps.\n",
    "      label: optional label used at logging steps.\n",
    "    \"\"\"\n",
    "    logger = loggers.TerminalLogger(label=label, time_delta=logger_time_delta)\n",
    "    iterator = range(num_episodes) if num_episodes else itertools.count()\n",
    "    all_returns = []\n",
    "\n",
    "    num_total_steps = 0\n",
    "    for episode in iterator:\n",
    "        # Reset any counts and start the environment.\n",
    "        start_time = time.time()\n",
    "        episode_steps = 0\n",
    "        episode_return = 0\n",
    "        episode_loss = 0\n",
    "\n",
    "        timestep = environment.reset()\n",
    "\n",
    "        # Make the first observation.\n",
    "        agent.observe_first(timestep)\n",
    "\n",
    "        # Run an episode.\n",
    "        while not timestep.last():\n",
    "            # Generate an action from the agent's policy and step the environment.\n",
    "            action = agent.select_action(timestep.observation)\n",
    "            timestep = environment.step(action)\n",
    "\n",
    "            # Have the agent observe the timestep and let the agent update itself.\n",
    "            agent.observe(action, next_timestep=timestep)\n",
    "            agent.update()\n",
    "\n",
    "            # Book-keeping.\n",
    "            episode_steps += 1\n",
    "            num_total_steps += 1\n",
    "            episode_return += timestep.reward\n",
    "\n",
    "            if num_steps is not None and num_total_steps >= num_steps:\n",
    "                break\n",
    "\n",
    "        # Collect the results and combine with counts.\n",
    "        steps_per_second = episode_steps / (time.time() - start_time)\n",
    "        result = {\n",
    "            \"episode\": episode,\n",
    "            \"episode_length\": episode_steps,\n",
    "            \"episode_return\": episode_return,\n",
    "        }\n",
    "        print(result)\n",
    "\n",
    "        all_returns.append(episode_return)\n",
    "\n",
    "        # Log the given results.\n",
    "        logger.write(result)\n",
    "\n",
    "        if num_steps is not None and num_total_steps >= num_steps:\n",
    "            break\n",
    "    return all_returns"
   ],
   "metadata": {
    "id": "JNBubvjAr5SM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(environment, agent, evaluation_episodes):\n",
    "    frames = []\n",
    "\n",
    "    for episode in range(evaluation_episodes):\n",
    "        timestep = environment.reset()\n",
    "        episode_return = 0\n",
    "        steps = 0\n",
    "        while not timestep.last():\n",
    "            frames.append(environment.render(mode=\"rgb_array\"))\n",
    "\n",
    "            action = agent.select_action(timestep.observation)\n",
    "            timestep = environment.step(action)\n",
    "            steps += 1\n",
    "            episode_return += timestep.reward\n",
    "        print(f\"Episode {episode} ended with reward {episode_return} in {steps} steps\")\n",
    "    return frames"
   ],
   "metadata": {
    "id": "uq51O8PPsOIw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "id": "BhB764-YsftO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_loop(agent=agent, environment=environment, num_episodes=3)"
   ],
   "metadata": {
    "id": "7PUY_CJ8Je7-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate"
   ],
   "metadata": {
    "id": "bnjtp6Jcsg2U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display_video(evaluate(agent=agent, environment=environment, evaluation_episodes=1))"
   ],
   "metadata": {
    "id": "jmxMTZXjshnl"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}