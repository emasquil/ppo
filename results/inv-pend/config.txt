# Training
num_training_iterations = 50
timesteps_per_iteration = 2000
gae_lambda = 0.95
num_epochs = 10
batch_size = 32
learning_rate_params = {
    "annealing": True,
    "policy": {
        "initial_learning_rate": 1e-5,
        "last_learning_rate": 0,
    },
    "value": {
        "initial_learning_rate": 3e-5,
        "last_learning_rate": 0,
    },
    "annealing_duration": num_training_iterations * np.ceil(timesteps_per_iteration / batch_size) * num_epochs,
}  # if "annealing" = False then "initial_learning_rate" is taken as the steady value
clipping_ratio_threshold = 0.2
max_grad_norm = 0.5
discount = 0.99
kl_threshold = None  # if kl_threshold is None we're not using it for early stopping
# if providing a sigma value, the policy net will only predict the mean and we'll use this fixed value as std
policy_net_sigma = 0.05 * (environment_spec.actions.maximum - environment_spec.actions.minimum)
#policy_net_sigma = None
# if not, you need to provide this 2 parameters to set a proper scale for sigma prediction
# min_sigma = 1e-6
# init_sigma = 0.3 * (environment_spec.actions.maximum - environment_spec.actions.minimum)

# Network
policy_hidden_layers = [
    {"output_size": 64, "std": np.sqrt(2), "bias": 0},
    {"output_size": 64, "std": np.sqrt(2), "bias": 0},
]
policy_last_layer = {"output_size": 64, "std": 0.01, "bias": 0}
value_hidden_layers = [
    {"output_size": 64, "std": np.sqrt(2), "bias": 0},
    {"output_size": 64, "std": np.sqrt(2), "bias": 0},
]
value_last_layer = {"output_size": 64, "std": 1, "bias": 0}

# Logs
log_dir = "main_experiments"
experiment_name = "inv-pend-good"

# Keys
seed = 0
key = jax.random.PRNGKey(seed)
key, key_init_networks = jax.random.split(key)
key, key_sampling_policy = jax.random.split(key)
key_dataloader, key_replay_buffer = jax.random.split(key)

logger_freq = 200