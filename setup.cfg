[metadata]
name = ppo
version = 0.1
description = Simple PPO implementation on OpenAI GYM environment.
license = MIT
requires = 
    setuptools 
    wheel

[options]
packages = ppo
python_requires = >= 3.8
install_requires =
    dm-env==1.5
    gym==0.22.0
    pygame==2.1.2
    dm-acme[jax,tensorflow]==0.4.0
    dm-sonnet==2.0.0
    trfl==1.2.0
    imageio==2.4.0
    PyVirtualDisplay==3.0
    tqdm==4.63.0
    ipykernel==6.9.1
    trax==1.4.1
    PyVirtualDisplay==3.0
    imageio==2.4.0

[options.extras_require]
dev = 
    black==22.1.0
