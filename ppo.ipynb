{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmQoT8K6nJX_"
   },
   "source": [
    "# Proximal Policy Optimization (PPO) playground\n",
    "\n",
    "Notebook for running PPO on simple environments from OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAZM8OR8nm2-"
   },
   "source": [
    "## Install dependencies (only on Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9avxJbcjt8Oi"
   },
   "outputs": [],
   "source": [
    "# Installing our own implementation\n",
    "! git clone https://github.com/emasquil/ppo.git\n",
    "! pip install -e /content/ppo\n",
    "\n",
    "# Visualization stuff\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Load autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHET6yCkw4ds"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import itertools\n",
    "import time\n",
    "import tqdm.notebook as tq\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from acme import specs\n",
    "from trax.jaxboard import SummaryWriter\n",
    "import pyvirtualdisplay\n",
    "\n",
    "# Set up a virtual display for rendering.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "\n",
    "import ppo.dm_helper as helpers\n",
    "from ppo.agents import RandomAgent, VanillaPPO, general_advantage_estmation\n",
    "from ppo.env_wrapper import PendulumEnv\n",
    "from ppo.networks import PolicyNetwork, ValueNetwork\n",
    "from ppo.replay_buffers import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7_OrJQ0qls7"
   },
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOhF8c-AqlTa"
   },
   "outputs": [],
   "source": [
    "def display_video(frames, filename=\"temp.mp4\", frame_repeat=1):\n",
    "    \"\"\"Save and display video.\"\"\"\n",
    "    # Write video\n",
    "    with imageio.get_writer(filename, fps=60) as video:\n",
    "        for frame in frames:\n",
    "            for _ in range(frame_repeat):\n",
    "                video.append_data(frame)\n",
    "    # Read video and display the video\n",
    "    video = open(filename, \"rb\").read()\n",
    "    b64_video = base64.b64encode(video)\n",
    "    video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" ' 'src=\"data:video/mp4;base64,{0}\">').format(\n",
    "        b64_video.decode()\n",
    "    )\n",
    "    return IPython.display.HTML(video_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-40V1lKDlip"
   },
   "source": [
    "## Definitions\n",
    "\n",
    "Definition of all the parts used in the learning loop: environment, agent, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rltCiwH2FYWA"
   },
   "outputs": [],
   "source": [
    "environment = PendulumEnv()\n",
    "environment_spec = specs.make_environment_spec(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpJvcAOtF2t3"
   },
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "def policy_network(observations):\n",
    "    return PolicyNetwork((10, 20), environment_spec.actions, \"policy\")(observations)\n",
    "\n",
    "\n",
    "def value_network(observations):\n",
    "    return ValueNetwork((10, 20), \"value\")(observations)\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(1)\n",
    "\n",
    "agent = VanillaPPO(environment_spec.observations, policy_network, value_network, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kluuB3xPJd_u"
   },
   "source": [
    "## Interaction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNBubvjAr5SM"
   },
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    environment,\n",
    "    agent,\n",
    "    num_training_iterations=2,\n",
    "    num_epochs=2,\n",
    "    len_rollout=5,\n",
    "    log_dir=\"experiments\",\n",
    "    experiment_name=\"pendulum_0\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training loop\n",
    "    \"\"\"\n",
    "    # Initializing counters\n",
    "    all_returns = []\n",
    "    avg_reward = 0.0\n",
    "    writer = SummaryWriter(os.path.join(log_dir, experiment_name))\n",
    "    # Counter to keep track of the global timestep\n",
    "    t = 0\n",
    "\n",
    "    pbar = tq.tqdm(range(num_training_iterations), position=0)\n",
    "    pbar.set_description(\"Training loop iteration\")\n",
    "    for iteration in pbar:\n",
    "        # Annealing\n",
    "\n",
    "        # Rollout phase\n",
    "        # Reset any counts and start the environment.\n",
    "        agent.replay_buffer.clear()\n",
    "        timestep = environment.reset()\n",
    "        rollout_return = 0\n",
    "\n",
    "        # Make the first observation.\n",
    "        agent.observe_first(timestep)\n",
    "\n",
    "        pbar_rollout = tq.tqdm(range(len_rollout), position=1, leave=False)\n",
    "        pbar_rollout.set_description(f\"Rollout step\")\n",
    "        for rollout_step in pbar_rollout:\n",
    "            pbar_rollout.set_postfix(reward=timestep.reward)\n",
    "            value = agent.get_value(timestep.observation)\n",
    "            action, log_prob = agent.select_action_and_prob(timestep.observation)\n",
    "            timestep = environment.step(action)\n",
    "            agent.observe(value, log_prob, action, timestep)\n",
    "\n",
    "            rollout_return += timestep.reward\n",
    "            # Incrementing count of time step\n",
    "            t += 1\n",
    "            # Avg reward per global timestep\n",
    "            avg_reward += (timestep.reward - avg_reward) / t\n",
    "            # Add avg reward\n",
    "            writer.scalar(\"return/avg_per_timestep\", avg_reward, t)\n",
    "\n",
    "            # Have the agent observe the timestep and let the agent update itself.\n",
    "            if timestep.last():\n",
    "                timestep = environment.reset()\n",
    "                agent.observe_first(timestep)\n",
    "                all_returns.append(rollout_return)\n",
    "                rollout_return = 0\n",
    "\n",
    "        # Learning phase\n",
    "        dataloader = DataLoader(agent.replay_buffer, len_rollout)\n",
    "        trajectory = dataloader.get_full_memory()\n",
    "        advantages = general_advantage_estmation(trajectory, agent, timestep)\n",
    "        agent.add_advantage(advantages)\n",
    "\n",
    "        # Add cummulative return of the episode to tensorboard\n",
    "        writer.scalar(\"return/cummulative_per_rollout\", rollout_return, iteration)\n",
    "\n",
    "        pbar_epochs = tq.tqdm(range(num_epochs), leave=False, position=1)\n",
    "        pbar_epochs.set_description(f\"Epoch\")\n",
    "        for epoch in pbar_epochs:\n",
    "            dataloader.shuffle()\n",
    "            value_losses = []\n",
    "            policy_losses = []\n",
    "            for batch in dataloader:\n",
    "                value_loss, policy_loss = agent.update(batch)\n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "            writer.scalar(\"loss/value\", np.mean(value_losses), epoch + iteration * num_epochs)\n",
    "            writer.scalar(\"loss/policy\", np.mean(policy_losses), epoch + iteration * num_epochs)\n",
    "\n",
    "        all_returns.append(rollout_return)\n",
    "    return all_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uq51O8PPsOIw"
   },
   "outputs": [],
   "source": [
    "def evaluate(environment, agent, evaluation_episodes):\n",
    "    frames = []\n",
    "\n",
    "    pbar = tq.tqdm(range(evaluation_episodes))\n",
    "    pbar.set_description(\"Episode\")\n",
    "    for episode in pbar:\n",
    "        timestep = environment.reset()\n",
    "        episode_return = 0\n",
    "        steps = 0\n",
    "        while not timestep.last():\n",
    "            frames.append(environment.render(mode=\"rgb_array\"))\n",
    "\n",
    "            action = agent.select_action(timestep.observation)\n",
    "            timestep = environment.step(action)\n",
    "            steps += 1\n",
    "            episode_return += timestep.reward\n",
    "        print(f\"Episode {episode} ended with reward {episode_return} in {steps} steps\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhB764-YsftO"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PUY_CJ8Je7-"
   },
   "outputs": [],
   "source": [
    "training_loop(agent=agent, environment=environment, len_rollout=20, num_training_iterations=10, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnjtp6Jcsg2U"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmxMTZXjshnl"
   },
   "outputs": [],
   "source": [
    "display_video(evaluate(agent=agent, environment=environment, evaluation_episodes=5))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ppo.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "98f9930fe2848a2b33a267fbaf3bc684d213d3ea326e502697c70dc4c5de83f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
