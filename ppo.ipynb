{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/emasquil/ppo/blob/main/ppo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmQoT8K6nJX_"
   },
   "source": [
    "# Proximal Policy Optimization (PPO) playground\n",
    "\n",
    "Notebook for running PPO on simple environments from OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAZM8OR8nm2-"
   },
   "source": [
    "## Install dependencies (only on Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9avxJbcjt8Oi"
   },
   "outputs": [],
   "source": [
    "# Installing our own implementation\n",
    "! git clone https://github.com/emasquil/ppo.git\n",
    "! pip install -e /content/ppo\n",
    "\n",
    "# Visualization stuff\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "\n",
    "# Dependencies needed for running mujoco on colab\n",
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf\n",
    "\n",
    "!pip install free-mujoco-py\n",
    "!pip install imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Load autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FHET6yCkw4ds"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/theovincent/MVA/DeepReinforcementLearning/ppo/env/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
      "  warnings.warn('jax.experimental.optimizers is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import tqdm.notebook as tq\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from acme import specs\n",
    "from trax.jaxboard import SummaryWriter\n",
    "import pyvirtualdisplay\n",
    "\n",
    "# Set up a virtual display for rendering.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "\n",
    "from ppo.agents import VanillaPPO, general_advantage_estimation\n",
    "from ppo.env_wrapper import PendulumEnv, ReacherEnv\n",
    "from ppo.networks import PolicyNetwork, ValueNetwork\n",
    "from ppo.replay_buffers import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7_OrJQ0qls7"
   },
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SOhF8c-AqlTa"
   },
   "outputs": [],
   "source": [
    "def display_video(frames, filename=\"temp.mp4\", frame_repeat=1):\n",
    "    \"\"\"Save and display video.\"\"\"\n",
    "    # Write video\n",
    "    with imageio.get_writer(filename, fps=60) as video:\n",
    "        for frame in frames:\n",
    "            for _ in range(frame_repeat):\n",
    "                video.append_data(frame)\n",
    "    # Read video and display the video\n",
    "    video = open(filename, \"rb\").read()\n",
    "    b64_video = base64.b64encode(video)\n",
    "    video_tag = (\n",
    "        '<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
    "        'src=\"data:video/mp4;base64,{0}\">'\n",
    "    ).format(b64_video.decode())\n",
    "    return IPython.display.HTML(video_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-40V1lKDlip"
   },
   "source": [
    "## Definitions\n",
    "\n",
    "Definition of all the parts used in the learning loop: environment, agent, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rltCiwH2FYWA"
   },
   "outputs": [],
   "source": [
    "environment = ReacherEnv()\n",
    "environment_spec = specs.make_environment_spec(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_training_iterations = 5\n",
    "max_rollout = 200\n",
    "gae_lambda =  0.95\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate_params = {\n",
    "    \"annealing\": True,\n",
    "    \"initial_learning_rate\": 1e-3, \n",
    "    \"last_learning_rate\": 1e-6, \n",
    "    \"annealing_duration\": num_training_iterations * np.ceil(max_rollout / batch_size) * num_epochs\n",
    "} # if \"annealing\" = False then \"initial_learning_rate\" is taken as the steady value\n",
    "clipping_ratio_threshold = 0.2\n",
    "max_grad_norm = 0.5\n",
    "discount = 0.99 \n",
    "kl_threshold = 1.5\n",
    "\n",
    "# Network\n",
    "policy_hidden_layers = [\n",
    "    {\"output_size\": 64, \"std\": np.sqrt(2), \"bias\": 0},\n",
    "    {\"output_size\": 64, \"std\": np.sqrt(2), \"bias\": 0},\n",
    "]\n",
    "policy_last_layer = {\"output_size\": 64, \"std\": 0.01, \"bias\": 0}\n",
    "value_hidden_layers = [\n",
    "    {\"output_size\": 64, \"std\": np.sqrt(2), \"bias\": 0},\n",
    "    {\"output_size\": 64, \"std\": np.sqrt(2), \"bias\": 0},\n",
    "]\n",
    "value_last_layer = {\"output_size\": 64, \"std\": 1, \"bias\": 0}\n",
    "\n",
    "# Logs\n",
    "log_dir = \"experiments\"\n",
    "experiment_name = \"pendulum_0\"\n",
    "\n",
    "# Keys\n",
    "key_networks = 1\n",
    "key_sampling_policy = 1\n",
    "key_shuffling_batch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xpJvcAOtF2t3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Create the agent\n",
    "def policy_network(observations):\n",
    "    return PolicyNetwork(\n",
    "        policy_hidden_layers, policy_last_layer, environment_spec.actions, \"policy\"\n",
    "    )(observations)\n",
    "\n",
    "\n",
    "def value_network(observations):\n",
    "    return ValueNetwork(value_hidden_layers, value_last_layer, \"value\")(observations)\n",
    "\n",
    "\n",
    "agent = VanillaPPO(\n",
    "    observation_spec=environment_spec.observations,\n",
    "    policy_network=policy_network,\n",
    "    value_network=value_network,\n",
    "    key_networks=key_networks,\n",
    "    key_sampling_policy=key_sampling_policy,  \n",
    "    learning_rate_params=learning_rate_params,\n",
    "    discount=discount, \n",
    "    clipping_ratio_threshold=clipping_ratio_threshold, \n",
    "    max_grad_norm=max_grad_norm\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kluuB3xPJd_u"
   },
   "source": [
    "## Interaction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JNBubvjAr5SM"
   },
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    environment,\n",
    "    agent,\n",
    "    num_training_iterations,\n",
    "    num_epochs,\n",
    "    max_rollout,\n",
    "    batch_size,\n",
    "    log_dir,\n",
    "    experiment_name,\n",
    "    kl_threshold\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training loop\n",
    "    \"\"\"\n",
    "    # Initializing counters\n",
    "    all_returns = []\n",
    "    avg_reward = 0.0\n",
    "    writer = SummaryWriter(os.path.join(log_dir, experiment_name))\n",
    "    # Counter to keep track of the global timestep\n",
    "    t = 0\n",
    "\n",
    "    pbar = tq.tqdm(range(num_training_iterations), position=0)\n",
    "    pbar.set_description(\"Training loop iteration\")\n",
    "    for iteration in pbar:\n",
    "        # Rollout phase\n",
    "        # Reset any counts and start the environment.\n",
    "        agent.replay_buffer.clear()\n",
    "        timestep = environment.reset()\n",
    "        rollout_return = 0\n",
    "\n",
    "        # Make the first observation.\n",
    "        agent.observe_first(timestep)\n",
    "\n",
    "        pbar_rollout = tq.tqdm(range(max_rollout), position=1, leave=False)\n",
    "        pbar_rollout.set_description(f\"Rollout step\")\n",
    "        for rollout_step in pbar_rollout:\n",
    "            pbar_rollout.set_postfix(reward=timestep.reward)\n",
    "            \n",
    "            value = agent.get_value(timestep.observation)\n",
    "            action, log_prob = agent.select_action_and_prob(timestep.observation)\n",
    "            timestep = environment.step(action)\n",
    "            agent.observe(value, log_prob, action, timestep)\n",
    "            rollout_return += timestep.reward\n",
    "            # Incrementing count of time step\n",
    "            t += 1\n",
    "            # Avg reward per global timestep\n",
    "            avg_reward += (timestep.reward - avg_reward) / t\n",
    "            \n",
    "            # Add to logs\n",
    "            writer.scalar(\"per_timestep/avg_reward\", avg_reward, t)\n",
    "            writer.scalar(\"per_timestep/reward\", timestep.reward, t)\n",
    "            writer.scalar(\"per_timestep/action[0]\", action[0], t)\n",
    "            writer.scalar(\"per_timestep/action[1]\", action[1], t)\n",
    "            writer.scalar(\"per_timestep/training_iteration\", iteration, t)\n",
    "\n",
    "            if timestep.last():\n",
    "                pbar_rollout.disp(close=True)\n",
    "                break\n",
    "\n",
    "        # Add cummulative return of the episode to tensorboard\n",
    "        writer.scalar(\"per_timestep/cummulative_per_rollout\", rollout_return, iteration)\n",
    "\n",
    "        # Learning phase\n",
    "        dataloader = DataLoader(agent.replay_buffer, batch_size, key_shuffling_batch)\n",
    "        trajectory = dataloader.get_full_memory()\n",
    "        advantages = general_advantage_estimation(\n",
    "            trajectory, agent, timestep, discount, gae_lambda\n",
    "        )\n",
    "        agent.add_advantage(advantages)\n",
    "\n",
    "        pbar_epochs = tq.tqdm(range(num_epochs), leave=False, position=1)\n",
    "        pbar_epochs.set_description(f\"Epoch\")\n",
    "        for epoch in pbar_epochs:\n",
    "            dataloader.shuffle()\n",
    "            value_losses = []\n",
    "            policy_losses = []\n",
    "            for batch in dataloader:\n",
    "                value_loss, policy_loss, kl_approximation = agent.update(batch)\n",
    "                value_losses.append(value_loss)\n",
    "                policy_losses.append(policy_loss)\n",
    "\n",
    "            writer.scalar(\"per_epoch/value_loss\", np.mean(value_losses), epoch + iteration * num_epochs)\n",
    "            writer.scalar(\"per_epoch/policy_loss\", np.mean(policy_losses), epoch + iteration * num_epochs)\n",
    "            writer.scalar(\"per_epoch/kl_divergence\", kl_approximation, epoch + iteration * num_epochs)\n",
    "            writer.scalar(\"per_epoch/learning_rate\", agent.get_learning_rate())\n",
    "            writer.scalar(\"per_epoch/training_iteration\", iteration, epoch + iteration * num_epochs)\n",
    "\n",
    "            if kl_threshold is not None and kl_approximation > kl_threshold:\n",
    "                pbar_epochs.disp(close=True)\n",
    "                break\n",
    "        all_returns.append(rollout_return)\n",
    "    return all_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uq51O8PPsOIw"
   },
   "outputs": [],
   "source": [
    "def evaluate(environment, agent, evaluation_episodes):\n",
    "    frames = []\n",
    "\n",
    "    pbar = tq.tqdm(range(evaluation_episodes))\n",
    "    pbar.set_description(\"Episode\")\n",
    "    for episode in pbar:\n",
    "        timestep = environment.reset()\n",
    "        episode_return = 0\n",
    "        steps = 0\n",
    "        while not timestep.last():\n",
    "            frames.append(environment.render(mode=\"rgb_array\"))\n",
    "            action = agent.select_action(timestep.observation)\n",
    "            timestep = environment.step(action)\n",
    "            steps += 1\n",
    "            episode_return += timestep.reward\n",
    "        print(f\"Episode {episode} ended with reward {episode_return} in {steps} steps\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20856), started 2:36:22 ago. (Use '!kill 20856' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e91fb08b9423a8f5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e91fb08b9423a8f5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhB764-YsftO"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7PUY_CJ8Je7-"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31238f885d346c1aa2d34433f018215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf43f71c56b34cb183c76b676db31e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fafb505a18244a7837dc9c3133ba2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theovincent/MVA/DeepReinforcementLearning/ppo/env/lib/python3.8/site-packages/jax/_src/random.py:366: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90155e9f8ba84ed3a73a8967c20e3a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d694f7f67bb4e7eabbd022f1f5ea3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5779072831d43e389c2e83540bd5ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c92011e6f247a9a60a7af4718c0e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fd031a48ff40609528728853dee009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e75c067ad8f42c19e1de54943efb42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b037ed203ab48ebb590bcd6540f021d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df38a0faacf04f13832d8284d8caa3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-17.288287285172043,\n",
       " -16.249705974959383,\n",
       " -12.532903110632716,\n",
       " -15.27096914874653,\n",
       " -14.888841592234206]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(agent=agent, environment=environment, max_rollout=max_rollout, batch_size=batch_size, num_training_iterations=num_training_iterations, num_epochs=num_epochs, log_dir=log_dir, experiment_name=experiment_name, kl_threshold=kl_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnjtp6Jcsg2U"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmxMTZXjshnl"
   },
   "outputs": [],
   "source": [
    "display_video(evaluate(agent=agent, environment=environment, evaluation_episodes=5))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ppo.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "98f9930fe2848a2b33a267fbaf3bc684d213d3ea326e502697c70dc4c5de83f2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
